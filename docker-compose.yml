services:
  uplink:
    build: .
    container_name: uplink-agent
    stdin_open: true # Keep stdin open for interactive input
    tty: true # Allocate a pseudo-TTY
    environment:
      # Load from .env file
      - GROQ_API_KEY=${GROQ_API_KEY}
      - BUNNY_API_KEY=${BUNNY_API_KEY}
      - BUNNY_STORAGE_ZONE_NAME=${BUNNY_STORAGE_ZONE_NAME}
      - BUNNY_STORAGE_ACCESS_KEY=${BUNNY_STORAGE_ACCESS_KEY}
      - BUNNY_PULLZONE_URL=${BUNNY_PULLZONE_URL}
    volumes:
      # Mount assets directory for file operations
      - ./assets:/app/assets
      - ./downloads:/app/downloads
      # Mount config files
      - ./mcp-config.json:/app/mcp-config.json:ro
      # Optional: mount .env file
      - ./.env:/app/.env:ro
    network_mode: host # For accessing localhost Ollama
    # Alternative if you want isolated networking:
    # ports:
    #   - "3000:3000"  # For graph-ui server
    # extra_hosts:
    #   - "host.docker.internal:host-gateway"  # Access host's Ollama

  # Optional: Include Ollama in the stack
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    # Uncomment if you have GPU
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

volumes:
  ollama-data:
